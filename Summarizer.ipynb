{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "import io\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    " \n",
    "def penn_to_wn(tag):\n",
    "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    " \n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    " \n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    " \n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    " \n",
    "    return None\n",
    " \n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    " \n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def levenshtein_distance(first, second):\n",
    "    \"\"\"Return the Levenshtein distance between two strings.\n",
    "    Based on:\n",
    "        http://rosettacode.org/wiki/Levenshtein_distance#Python\n",
    "    \"\"\"\n",
    "    if len(first) > len(second):\n",
    "        first, second = second, first\n",
    "    distances = range(len(first) + 1)\n",
    "    for index2, char2 in enumerate(second):\n",
    "        new_distances = [index2 + 1]\n",
    "        for index1, char1 in enumerate(first):\n",
    "            if char1 == char2:\n",
    "                new_distances.append(distances[index1])\n",
    "            else:\n",
    "                new_distances.append(1 + min((distances[index1],\n",
    "                                             distances[index1 + 1],\n",
    "                                             new_distances[-1])))\n",
    "        distances = new_distances\n",
    "    return distances[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentence_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    " \n",
    "    score, count = 0.1, 0\n",
    "    \n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "       \n",
    "        for ss in synsets2:\n",
    "            if(synset.path_similarity(ss) is not None):\n",
    "                best_score=max(score,synset.path_similarity(ss))\n",
    "            else:\n",
    "                best_score=score\n",
    "        # Check that the similarity could have been computed\n",
    "            if best_score is not None:\n",
    "                score += best_score\n",
    "                count += 1\n",
    "    if count is 0:\n",
    "        return 0\n",
    "    # Average the values\n",
    "    score /= count\n",
    "    if count is 0:\n",
    "        return 0\n",
    "    if score is not None:\n",
    "        return score\n",
    "    else:\n",
    "        return 0.0\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f= open('Article.txt', \"r\")\n",
    "\n",
    "all_text = f.read()\n",
    "temp=[]\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\",ignore_stopwords = True)\n",
    "words = all_text.split()\n",
    "words1 = []\n",
    "for i in range(len(words)):\n",
    "    words1.append(stemmer.stem(words[i]))\n",
    "afterstem = \" \".join(words1)\n",
    "temp.append(afterstem)\n",
    "#print(temp)\n",
    "str = ''.join(temp)\n",
    "str=''.join(all_text)\n",
    "#print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(nodes):\n",
    "    \"\"\"Return a networkx graph instance.\n",
    "    :param nodes: List of hashables that represent the nodes of a graph.\n",
    "    \"\"\"\n",
    "    gr = nx.Graph()  # initialize an undirected graph\n",
    "    gr.add_nodes_from(nodes)\n",
    "    nodePairs = list(itertools.combinations(nodes, 2))\n",
    "\n",
    "    # add edges to the graph (weighted by Levenshtein distance)\n",
    "    for pair in nodePairs:\n",
    "        firstString = pair[0]\n",
    "        secondString = pair[1]\n",
    "        levDistance = levenshtein_distance(firstString, secondString)\n",
    "        \n",
    "        WNScore=sen(firstString, secondString)\n",
    "        \n",
    "        Distance=(levDistance+WNScore)/2.0\n",
    "        gr.add_edge(firstString, secondString, weight=Distance)\n",
    "\n",
    "    return gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def sen(first,second):\n",
    "    score, count = 0.0,0\n",
    "    for word in first:\n",
    "        for each in second:\n",
    "            x=wordnet.synsets(word)[0]\n",
    "            y=wordnet.synsets(each)[0]\n",
    "            if(x and y):\n",
    "                score+=x.wup_similarity(y)\n",
    "                count+=1\n",
    "                if count:\n",
    "                    return score/count\n",
    "                else:\n",
    "                    return 0\n",
    "            else:\n",
    "                return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_sentences(text,l):\n",
    "    \n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentence_tokens = sent_detector.tokenize(text.strip())\n",
    "    graph = build_graph(sentence_tokens)\n",
    "    calculated_page_rank = nx.pagerank(graph, weight='weight')\n",
    "    \n",
    "    # most important sentences in ascending order of importance\n",
    "    sentences = sorted(calculated_page_rank, key=calculated_page_rank.get,\n",
    "                       reverse=True)\n",
    "    l=int(l)\n",
    "    # return a 100 word summary\n",
    "    summary = ' '.join(sentences)\n",
    "    summary_words = summary.split()\n",
    "    \n",
    "    summary_words = summary_words[0:l]\n",
    "    \n",
    "    summary = ' '.join(summary_words)\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "def extract_sentences1(text,l):\n",
    "    \n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentence_tokens = sent_detector.tokenize(text.strip())\n",
    "    graph = build_graph(sentence_tokens)\n",
    "    calculated_page_rank = nx.pagerank(graph, weight='weight')\n",
    "    \n",
    "    # most important sentences in ascending order of importance\n",
    "    sentences = sorted(calculated_page_rank, key=calculated_page_rank.get,\n",
    "                       reverse=True)\n",
    "    l=int(l)\n",
    "    # return a 100 word summary\n",
    "    summary = ' '.join(sentences)\n",
    "    \n",
    "    x=tokenize.sent_tokenize(summary)\n",
    "    x=x[0:3]\n",
    "    x=x[:-1]\n",
    "    print(x)\n",
    "   \n",
    "\n",
    "    summary = ' '.join(x)\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If you need to download an application that does not exist in the Play Store, try to go for the reputed App stores like Amazon App Store or APKMirror, otherwise, avoid downloading it and look for better alternatives.', 'So the chance of a malicious app making to the app store is negligible and by any means, if the application does reach the app store, Google keeps a track and will take down the app as soon as possible.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "count=len(re.findall(r'\\w+', str))\n",
    "count=int(count/3)\n",
    "for each in range(len(str)-count):\n",
    "    if str[count+each] is '.':\n",
    "        break\n",
    "\n",
    "summary = extract_sentences1(str,count)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
